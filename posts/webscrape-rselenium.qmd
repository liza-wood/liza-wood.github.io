---
title: "Untitled"
---

R Selenium

https://www.zenrows.com/blog/rselenium#export-data
https://www.scrapingbee.com/blog/getting-started-with-rselenium/
https://www.geeksforgeeks.org/web-scraping-using-rselenium/

```{r}
library(RSelenium)
library(xml2)
library(rvest)
library(dplyr)
```


Organix seed database for the European Union.

Selenium is different than standard scraping because it is a 'headless browser' -- you can click through and mimic human activity more readily

# Set up 

I always run a Docker container to host Selenium on. We can open Docker and start the container from the Terminal, via the `system` function in R.

```{r}
system("open -a Docker")
system('docker run --name organix -d -p 4445:4444 -p 5901:5900 selenium/standalone-firefox-debug')
```

Then I need to define the remot driver
```{r}
remDr <- remoteDriver(
  remoteServerAddr = "localhost",
  port = 4445L,
  browserName = "firefox",
  path = '/wd/hub'
)
```

And then you can view the headless browser if you want to watch its activity:

```{r}
system('open vnc://127.0.0.1:5901') # password: secret
```

But then you will use the remDr object to execute commands. The first command is to connect to the remote server. 
```{r, warning = F, message = F}
remDr$open()
```
And navigate to the webpage

```{r}
organix_url <- "https://www.organicxseeds.com/international/countryselect"
remDr$navigate(organix_url)
```

 
 From here, we can do several things

# Regular scraping

You can just take the HTML without doing anything fancy.

```{r}
html <- remDr$getPageSource()[[1]]
```

Then you can use normal `xml2` functions to

```{r}
read_html(html) %>% 
  html_nodes(css = '.selectLink') %>% 
  html_text(trim=TRUE)

urls <- read_html(html) %>% 
 html_nodes(css = '.selectLink')%>% 
 html_attr("href")
urls 
```

parse information from the html. In this case, there was little need for RSelenium at all, given that we just could have taken the html from this page with the following:

```{r}
read_html(organix_url) %>% 
  html_nodes(css = '.selectLink') %>% 
  html_text(trim=TRUE)
```

However, to explore this page further, we'll need to use RSelenium. This is because some pages, like the Belgium page, redirects to the country selection page. 

```{r}
remDr$navigate("https://www.organicxseeds.be/")
```

So, we will want to use RSelenium to manually click through some links to be sure not to be redirected;

# Clicking through

https://stackoverflow.com/questions/29831142/how-can-i-click-a-link-in-a-webpage-in-rselenium

I find this useful especially if links get redirected, or (for reasons we'll see with skipping through), you need to click different elements of a page to expand them out.

For the redirection, we'll see that some countries (Belgium and Switzerland), routing to their pages (and their supplier lists) brings us back to the original country list

So ideally, we'd use the base URLs to then get us their suppliers list
```{r}
remDr$navigate("https://www.organicxseeds.nl/search/listsuppliersincountry")
```

But in other cases it does not:
```{r}
remDr$navigate("https://www.organicxseeds.ch/search/listsuppliersincountry")
```

So in these cases, we will click through manually by using the `findElement` function. Elements can include the following 'arg' argument: "xpath", "css selector", "id", "name", "tag name", "class name", "link text", "partial link text".

In this case, we will use the 'italiano' link text because it is unique to the Swiss site. Then we will `clickElement`

```{r}
webElem <- remDr$findElement(using = 'link text',"italiano")
webElem$clickElement()
```

Then we can continue to navigate to the supplier lists, again using link text. From studying the sites, we know this text is "Mostra tutti i offerente"

```{r}
webElem <- remDr$findElement(using = 'link text',"Mostra tutti i offerente")
webElem$clickElement()
```

Now we've made it to the desired site, and then we can scrape suppliers from there.

In any case, we can then use rvest to get the suppliers from each country

```{r}
html <- remDr$getPageSource()[[1]]

suppl <- read_html(html) %>% 
  html_nodes(css = "tbody a") %>%
  html_attr("href") %>% 
  trimws()
suppl
```


We can then iterate through the 

```{r}
ids_end <- c("be", "dk", "de", "is", "lt", "lu", "nl", "ch", "co.uk")
ids_start <- c("ie", "se", "nir")
ids <- c(ids_end, ids_start)
suppliers <- list()

for(i in 1:length(ids)){
  if(ids[i] %in% ids_end){
    url <- paste0("https://www.organicxseeds.", ids[i], "/search/listsuppliersincountry")
  } else {
    url <- paste0("https://", ids[i], ".organicxseeds.com/search/listsuppliersincountry")
  }
  if(i == 1){
    # The Belgian extension just redirects, so need to click through
    remDr$navigate("https://www.organicxseeds.com/international/countryselect")
    webElem <- remDr$findElement(using = 'link text',"deutsch")
    webElem$clickElement()
    webElem <- remDr$findElement(using = 'link text',"Alle Anbieter anzeigen")
    webElem$clickElement()
  } else if(i == 8){
    # The Swiss extension just redirects, so need to click through
    remDr$navigate("https://www.organicxseeds.com/international/countryselect")
    webElem <- remDr$findElement(using = 'link text',"italiano")
    webElem$clickElement()
    webElem <- remDr$findElement(using = 'link text',"Mostra tutti i offerente")
    webElem$clickElement()
  } else {
    remDr$navigate(url)
  }
  
  html <- remDr$getPageSource()[[1]]
  Sys.sleep(1)
  suppl <- read_html(html) %>% 
    html_nodes(css = "tbody a") %>%
    html_attr("href") %>% 
    trimws()
  suppliers[[i]] <- suppl
  names(suppliers)[i] <- ids[i]
}
lengths(suppliers)
```



# Screenshots
 <!--- eu_organix.R in osisn_spatial, appendix, global snapshot this is the worse version because it relies on screenshots ---->
 
I find R Selenium most useful when dealing with data that is stored behind the site as java, but you don't have APIs.

Examples could be when certificates are posted online: Organix and Integrity are two good examples of this.

How to over come this? It isn't pretty, but... screenshots

```{r}
supply_sites <- unlist(suppliers)
supply_sites[1]
```


## Zooming in/out
https://stackoverflow.com/questions/60139836/zoom-out-of-website-when-using-rselenium-without-changing-page-size-resolution

## Scrolling

https://stackoverflow.com/questions/31901072/scrolling-page-in-rselenium

# Unfolding
 <!--- organix_for_cvpo.R in osisn_spatial, appendix, validating with co-development, get_data this is the better version ---->

## Login?


```{r}
remDr$close()
system('docker stop organix')
system('docker remove organix')
```

