[
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Liza Wood",
    "section": "",
    "text": "l.wood3@exeter.ac.uk |  website\n\n\n\nEDUCATION\nPh.D., Environmental Policy emphasis in Ecology 2023\nSecondary emphasis: Computational Social Science\nUniversity of California Davis, Davis, CA\nM.Sc., Organic Farming and Food Production Systems 2015\nNewcastle University, Newcastle upon Tyne, United Kingdom\nM.Sc., Sustainability Science and Policy 2014\nMaastricht University, Maastricht, Netherlands\nB.A., Political Science | B.A., Biology 2013\nHonors College at the College of Charleston, Charleston, SC\n\n\n\nEMPLOYMENT\nPostdoctoral Research Fellow 2023 – present\nUniversity of Exeter, Department of Management – Sustainable Futures\nGraduate Student Researcher and Teaching Assistant 2018 – 2023\nUniversity of California Davis, Center for Environmental Policy & Behavior and DataLab\nAssistant Director of First-Year Experience 2016 – 2018\nCollege of Charleston, New Student Programs\n\n\n\nPUBLICATIONS\n\nPEER-REVIEW PUBLICATIONS\nScott, T., Kim, S., Wood, L. (2025) Open access, professional popularity, and agency involvement drive science referenced in U.S. EPA regulatory impact analyses from 2000 to 2022. Regulation & Governance. http://doi.org/10.1111/rego.70026\nWood, L., Vantaggiato, F.P., Scott, T.A. (2025) Tracing the path of knowledge on ‘environmental governance processes’ for theory-building. Environmental Policy and Governance. https://doi.org/10.1002/eet.2151\nWood, L., Lubell, M. (2025) Resource constellations and institutional logics shape network-building processes in the organic seed niche. Environmental Innovation and Societal Transitions. https://doi.org/10.1016/j.eist.2025.100965\nWood, L., Scott, T. (2022) Transportation agencies as consumers and producers of science: The case of state, regional, and county transportation agencies in California. Transport Policy 128. https://doi.org/10.1016/j.tranpol.2022.08.016.\nWood, L., Lubell, M., Rudnick, J., Khalsa, S.D.S., Tatge, S., Sears, M., Brown, P. (2021) Mandatory information-based policy tools facilitate California farmers’ learning about nitrogen management. Land Use Policy 114. https://doi.org/10.1016/j.landusepol.2021.105923.\nRudnick, J., Lubell, M., Khalsa, S.D.S., Tatge, S., Wood, L., Sears, M., Brown, P. (2021). A farm systems approach to the adoption of sustainable nitrogen management practices in California. Agriculture and Human Values 38. https://doi.org/10.1007/s10460-021-10190-5.\n\n\nCONFERENCE PAPERS\nJenney, P., Wood, L., Dawson, J., Hubbard, K. When the Patent Bargain Fails: An examination of patents on plant varieties and the consequences for follow-on innovation. Rural Sociological Society Annual Meeting (2025).\nWood, L. How public and private innovations shape environmental sustainability: Evidence from a century of plant patents. Research Policy 8th Online Conference for Early Career Researchers (2024).\nWood, L., Pascucci, S. Entanglements of ecological systems and organizational transformation in the organic seed niche. European Group for Organizational Studies (2024).\nWood, L., Pascucci, S. Mapping the research landscape of collaborative processes in sustainable business model innovation. King’s College London Centre for Sustainable Business Short Paper Workshop: Unlocking pathways for a sustainable future (2024)\nWood, L., Struthers, C., Scott, T.A., Kim, S. Science politics: How political context shapes public agencies’ use of science in environmental policymaking. Environmental Politics and Governance Conference (2023).\n\n\nREPORTS\nWood, L., Pascucci, S., Foppe, A., Ashouri, S., Materia, V., Alerasoul, A. (2024). How to design sustainable business models: A research integrated protocol. Deliverable 5.1 for InBestSoil: Monetary valuation of soil ecosystem services and creation of initiatives to invest in soil health.\nWood, L. (2022). Assessing the Resilience of the Organic Seed System: A Network Perspective. Proceedings of the 11th Organic Seed Growers Conference.\nHubbard, K., Zystro, J., & Wood, L. (2022). State of Organic Seed. Port Townsend, Washington. | Data explorer: https://organicseed.shinyapps.io/SOSData/\nWood, L., Lubell, M. (2022). Vina Subbasin Farmer Survey Report | Data explorer: https://liza-wood.shinyapps.io/Vina-interactive-survey/\n\n\n\n\nPRESENTATIONS\n\nCONFERENCE PRESENTATIONS\nRural Sociological Society Annual Meeting | Utah USA 2025\nEuropean Forum for Studies of Policies for Research and Innovation | Germany 2025\nResearch Policy 8th Online Conference for Early Career Researchers | virtual 2024\nEuropean Group for Organizational Studies | Italy 2024\nEnvironmental Politics and Governance Conference | United Kingdom, Canada 2023, 2024\nKing’s College London Centre for Sustainable Business Short Paper Workshop | United Kingdom 2024\nInternational Sustainability Transitions Conference | virtual, Netherlands 2022, 2023\nConference of Policy Process Research | Colorado USA 2023\nDuck Family Workshop on Environmental Politics and Governance | Washington USA 2022\nAtlanta Academy on Science and Innovation Policy | Georgia USA 2022\nAmerican Association of Geographers Annual Meeting | virtual 2020\nInternational Association for the Study of the Commons | Peru 2019\n\n\nTALKS, WORKSHOPS, AND DISCUSSANT ROLES\nPolitecnico di Bari, Management Group April 2025\n  Invited talk | Who innovates for climate adaptation? Evidence from plant variety development\nLondon Environment and Public Policy Workshop September 2024\n  Invited discussant | Panel: Palace politics in climate and environmental governance\nKing’s College London, Environment and Public Policy Research Group Seminar January 2024\n  Seminar talk | Tracing the path of knowledge on ‘environmental governance processes’ for theory-building\nUniversity of California Davis, Data Visualization ECL 290 October 2023\n  Invited lecture | Interactive data with Shiny flexdashboard\nUniversity of California Davis, Mexican Graduate Student Association May 2023\n  Invited workshop | Introduction to the Tidyverse & Making websites with Rmd & GitHub\nUniversity of California Davis, DataLab Micro-credential Program May 2023\n  Invited workshop | Network visualization\nNewcastle University, School of Natural and Environmental Sciences Seminar Series June 2022\n  Invited talk | Supporting innovation for sustainable agriculture\nMars-Wrigley Cacao Research and Development Seminar Series December 2021\n  Invited talk | Mapping the seed innovation system\n\n\n\n\nTEACHING EXPERIENCE\n\nDATA SCIENCE\nInstructor | Contracted by California State Water Board\nR for Water Resources Data Science Spring 2023, 2024 & 2025\nTeaching Assistant | UC Davis – UC Davis DataLab\nIST 8x: Adventures in Data Science Winter & Spring 2022 & 2023\nInstructor | UC Davis – Graduate Group in Ecology\nECL 298: R Data Analysis for Visualization in Science Fall 2020, 2021 & 2022\n\n\nENVIRONMENTAL AND SUSTAINABILITY SCIENCE\nTeaching Assistant | UC Davis – Departments of Plant Sciences and Environmental Science and Policy\nPLS 49: Organic Crop Production Practicum Fall 2019, Spring 2020\nESP 10: Current Issues in the Environment Winter 2019\nInstructor | UC Davis – Graduate Group Ecology\nECL 290: Science Fiction and Near Futures of Ecology Spring 2020\nInstructor | College of Charleston – Department of Environmental and Sustainability Science\nFYET 177: Sustainable Food in the Foothills of the Andes Spring 2017, 2018 & 2019\nFYSE 142: Farm to Fork: The Sustainable Food Movement in Charleston Spring 2018\n\n\n\n\nGRANTS, FELLOWSHIPS & AWARDS\n\nRESEARCH\nUniversity of Exeter Department of Management Seed Funding (£1,000) 2024 – 2025\nUSDA Western Sustainable Agriculture Research & Education Graduate Grant ($24,997) 2020 – 2022\nUC Davis Henry A. Jastro Student Research Fellowship ($12,450) 2019 – 2023\nFulbright Netherlands Research Award, Semi-finalist 2022\nUC Davis Angela and Theodore Foin Graduate Fellowship ($900) 2020 – 2021\nFord Foundation Fellowship Program, Honorable Mention 2020\nDavis Muir Institute Graduate Fellowship ($5,000) 2019 – 2020\nCollege of Charleston Political Science Department, 1st place for Best Paper Award 2013\nASIANetwork/Freeman-Asia Foundation Student-Faculty Fellowship to Thailand ($6,000) 2012\nCollege of Charleston Summer Undergraduate Research with Faculty Grant ($5,000) 2011\n\n\nEDUCATION\nUC Davis Graduate Group in Ecology Fellowship 2018 – 2020\nRotary Foundation Global Grant Scholarship to Newcastle, United Kingdom 2014 – 2015\nFulbright Maastricht University Award to Maastricht, The Netherlands 2013 – 2014\nCollege of Charleston Bishop Robert Smith Award 2013\nCollege of Charleston Biology Outstanding Student in the Major Award 2013\nCollege of Charleston Political Science Outstanding Student in the Major Award 2013\nCollege of Charleston Maggie T. Pennington Scholarship 2012-2013\nCollege of Charleston Alexandria Dengate Scholarship 2011-2012\n\n\n\n\nSERVICE\nProgram Committee Member | Atlanta Conference on Science and Innovation Policy  2025\nDavis R Users Group Coordinator | UC Davis  2021 – 2023\nEcology Graduate Group Graduate Student Mentor | UC Davis  2019 – 2022\nData Carpentries Instructor | The Carpentries  2021 – 2022\nAdmissions Committee Application Reviewer | UC Davis  2019 – 2021\nSustainability Literacy Writing Committee Member | College of Charleston  2017 – 2018\nFaculty for Sustainable Living Workshop Coordinator | College of Charleston  2016 – 2018\nInternal Fellowship Interview Panelist | College of Charleston  2016 – 2017\nReviewer | Policy Studies Journal, Society & Natural Resources, Environmental Innovations and Societal Transitions, Frontiers in Agronomy, European Management Review"
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops & Tutorials",
    "section": "",
    "text": "Introduction to the Tidyverse\nAn introductory workshop on essential data cleaning and visualizing pipelines using the tidy workflow. This was a workshop designed for the UC Davis Mexican Graduate Student Association.\n\nIntroduction to R programming videos\nThese videos, recorded with co-instructors Christian John, Tyler Scott, and Tara Pozzi, are available from each year of R-DAVIS instruction."
  },
  {
    "objectID": "workshops.html#introductory-r-programming",
    "href": "workshops.html#introductory-r-programming",
    "title": "Workshops & Tutorials",
    "section": "",
    "text": "Introduction to the Tidyverse\nAn introductory workshop on essential data cleaning and visualizing pipelines using the tidy workflow. This was a workshop designed for the UC Davis Mexican Graduate Student Association.\n\nIntroduction to R programming videos\nThese videos, recorded with co-instructors Christian John, Tyler Scott, and Tara Pozzi, are available from each year of R-DAVIS instruction."
  },
  {
    "objectID": "workshops.html#iteration-functional-programming",
    "href": "workshops.html#iteration-functional-programming",
    "title": "Workshops & Tutorials",
    "section": "Iteration & functional programming",
    "text": "Iteration & functional programming\nReal-world function writing\nA brief introduction to function-writing using the case of reading in US Patent Office .csv files and cleaning up the data to demonstrate the usefulness of combining functions and iteration. This was a presentation designed for for the Davis R-Users Group mini-workshops.\n\nIntroduction to creating a package with roxygen2\nBuilding off of the function-writing tutorial, this lesson outlines steps for turning a function into a package using the roxygen2 package. This was a presentation designed for for the Davis R-Users Group mini-workshops."
  },
  {
    "objectID": "workshops.html#webscraping-text-as-data",
    "href": "workshops.html#webscraping-text-as-data",
    "title": "Workshops & Tutorials",
    "section": "Webscraping & text as data",
    "text": "Webscraping & text as data\nWebscraping public records with rvest + regex intro\nA re-vamp of my first webscraping overview, this lesson demonstrates how to scrape data from public salary database, https://transparentcalifornia.com/. This was a presentation designed for for the Davis R-Users Group mini-workshops.\n\nTreating text as data with PDFs and regex\nA brief overview of working with text as data (web scraping, PDF reading, and OCR) and regular expressions (regex) for cleaning messy text data with an emphasis on the stringr package. This was a presentation designed for for the Davis R-Users Group mini-workshops."
  },
  {
    "objectID": "workshops.html#networks",
    "href": "workshops.html#networks",
    "title": "Workshops & Tutorials",
    "section": "Networks",
    "text": "Networks\nNetwork visualization with ggraph\nAn intermediate exploration of network data, including a brief review of storing and managing network data and objects, and a focus on integrating network summaries with visualization according to the grammar of graphics. This workshop was designed for the UC Davis DataLab’s micro-credential program.\n\nPreparing data for social-ecological network analysis with motifR\nAn overview on preparing social-ecological network for analysis in motifR along with the use of basic functions."
  },
  {
    "objectID": "workshops.html#creating-webpages",
    "href": "workshops.html#creating-webpages",
    "title": "Workshops & Tutorials",
    "section": "Creating webpages",
    "text": "Creating webpages\nIntroduction to flexdashboard with interactive data visualiation\nAn introduction to making interactive dashboards with flexdashboard and Shiny integration. This was a brief lesson designed for the UC Davis Ecology Graduate Group Data Visualization ECL 290.\n\nMaking webpages with Rmd + GitHub\nAn introduction to making web pages with GitHub Pages, with an emphasis on project management and using git from Terminal. This was a workshop designed for the UC Davis Mexican Graduate Student Association."
  },
  {
    "objectID": "workshops.html#statistics",
    "href": "workshops.html#statistics",
    "title": "Workshops & Tutorials",
    "section": "Statistics",
    "text": "Statistics\nOrdered logits introduction with MASS + VGAM\nA brief introduction to ordered logits using two different packages in R, reviewing the different model specifications and some of the more advances features of the VGAM package, particularly non-proportional and partial proportional models. Example using simulated farmer survey data.\n\nSequential ordered logits\nAdvancing on the ordered logit introduction, this provides an overview of the continuation ratio (also called sequential) model to ordered logits that want to analyze a dependent ordinal variable that constitutes sequential stages. Example using simulated farmer survey data."
  },
  {
    "objectID": "workshops.html#personalized-pan-docs",
    "href": "workshops.html#personalized-pan-docs",
    "title": "Workshops & Tutorials",
    "section": "Personalized pan-docs",
    "text": "Personalized pan-docs\nConverting Evernote notes to markdown files\nA quick overview of using the evernote2md package in R to convert .enex files from Evernote into more accessible .md files.\n\nConverting Google Docs to LaTeX template for aggiedown\nA workflow for connecting the aggiedown dissertation LaTeX template with Google Docs. The point is to allow dissertation-writing in Google Docs with Zotero plug-in with a direct feed into an Rmd and BibTeX format."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Liza Wood",
    "section": "",
    "text": "I am a Postdoctoral Research Fellow at the University of Exeter. I work with the EU Horizon/UKRI InBestSoil project, focusing on collaborative organizational strategies and business models for soil health. I earned my PhD at University of California Davis in the Environmental Policy specialization of the Ecology Graduate Group, with a designated emphasis in Computational Social Science. My research spans four areas: science in policymaking, innovation systems and networks, agri-environmental regulation, and innovation for the public good. I specialize in cases related to agri-food systems, ranging from agrobiodiversity management and organic certification to nitrogen policy. I also hold degrees in Sustainability Science and Policy (MSc), Organic Farming (MSc), and Biology and Political Science (BA).\nThis website includes workshops and tutorials in R that focus on computational and statistical topics. I am enthusiastic about teaching and publicly sharing code to promote data and statistical literacy, particularly for early learners."
  },
  {
    "objectID": "posts/llms-text-analysis.html",
    "href": "posts/llms-text-analysis.html",
    "title": "LLMs for text analysis",
    "section": "",
    "text": "Talk about Artificial Intelligence is everywhere. From university teaching, student essays, academic research, regulatory consultations, and even a whole array of unnecessary appliances like toothbrushes.\nThis post is about AI, and more specifically, Machine Learning (ML) in research. To be self-referential, claude.ai generated this overview of different ML methods and how they relate. ML covers a wide array of approaches for computer prediction based on ‘learning’ (i.e. giving the computer data). Learning is usually classified into three types: Supervised, Unsupervised, and Reinforcement.\n\n\n\nOverview of machine learning concepts, generated by claude.ai\n\n\nIn each of these categories, the figure highlights that various statistical methods that have been around for quite a while fall under the ML umbrella, such as statistical regression (supervised) and Principle Component Analysis (unsupervised). But beyond these traditional approaches, each of these types of learning can use ‘neural networks’ for deep learning.\nNeural networks – computational models inspired by neural networks in biology – are typically ‘trained’ (supervised) on data so that it learns how to minimize the difference in the output and the values in the training dataset. One popular software for implementing neural networks is (Keras.io)[https://keras.io/], allowing researchers to train and develop their own predictive models.\nThe ‘transformer’ architecture behind models like ChatGPT and Claude has evolved from these sorts of neural networks (but critically, rely on a ‘self-attention mechanism’ to predict word sequence). And after being trained on massive amounts of text1, the resulting Large Language Models (LLMs) are what we can chat with in either an unsupervised fashion (open chats) or supervised fashion (LLMs that are tuned to specific tasks).\n\n\nNow that LLMs have already been trained on huge amounts of data, researchers are able to leverage the learning that has already been done to help assist with research tasks. Researchers are using LLMs in a variety of ways, ranging from synthesizing social data to generating research ideas.\nThis post is specifically about text classification (and sometimes interpretation) – reading a text and grouping the text according to a decision criteria, or extracting some feature of the text. Specifically, I will be running through how to use OpenAI models for text classification tasks."
  },
  {
    "objectID": "posts/llms-text-analysis.html#llms-in-research",
    "href": "posts/llms-text-analysis.html#llms-in-research",
    "title": "LLMs for text analysis",
    "section": "",
    "text": "Now that LLMs have already been trained on huge amounts of data, researchers are able to leverage the learning that has already been done to help assist with research tasks. Researchers are using LLMs in a variety of ways, ranging from synthesizing social data to generating research ideas.\nThis post is specifically about text classification (and sometimes interpretation) – reading a text and grouping the text according to a decision criteria, or extracting some feature of the text. Specifically, I will be running through how to use OpenAI models for text classification tasks."
  },
  {
    "objectID": "posts/llms-text-analysis.html#api",
    "href": "posts/llms-text-analysis.html#api",
    "title": "LLMs for text analysis",
    "section": "API",
    "text": "API\nThis is outlined in this paper. When researchers are using these models, they are, by and large, using the API"
  },
  {
    "objectID": "posts/llms-text-analysis.html#chatgpt",
    "href": "posts/llms-text-analysis.html#chatgpt",
    "title": "LLMs for text analysis",
    "section": "ChatGPT",
    "text": "ChatGPT\nI have not heard of any researchers\nWhy not?\n\nIt doesn’t scale well – copy and paste, conversation limits, less control of parameter settings, harder to reproduce"
  },
  {
    "objectID": "posts/llms-text-analysis.html#getting-set-up",
    "href": "posts/llms-text-analysis.html#getting-set-up",
    "title": "LLMs for text analysis",
    "section": "Getting set up",
    "text": "Getting set up\nThis post is starting from the very basics, so we are going to walk through the following, step-by-step, to get a user set up. Specifically, I am going to walk through a set-up for a scenario where you may be working with a team with a shared API key.\n\nVisit https://platform.openai.com/\n\nSign up to create an account (or log in if you already have an account via ChatGPT)\n\nNavigate to your profile\n\nOrganization &gt; Members &gt; Invite (will need to name your ‘Organization’)\n\nNavigate to Projects &gt; Create\n\nAdd members to project\n\nWithin the project, navigate to API keys &gt; Create new secret key\n\nSet up billing\n\nSteps 1-2 are fairly self explanatory, so we will start with Step 3. On the upper right-hand corner of the page, once you are logged in, you can select your name/photo icon and select ‘Your profile’ to take you to the Settings of your profile.\n Once you are in your profile, it can be useful to set up an Organization if you plan to work with collaborators. On the left-hand side of the window, under the Organizations subheading, navigate to Members.\nYou can begin by clicking on the black + Invite button on the main page to invite members to the organization. This will prompt you to name your Organization, if you have not already. After naming your organization you will be able to invite members to the organization by adding their email addresses (Step 4).\nOnce you have created your organization, Step 5 is to create a Project for you and select members to work on. As OpenAI described, “Projects are shared environments where teams can collaborate and share API resources. You can set custom rate limits and manage access to resources.” To create one, again on the left-hand side navigate to Projects (under the Organization subheading) and clicking on the black + Create button. Give your project a name.\nOnce your project is created you can add members from your organization to a specific project (Step 6). Again on the left-hand side, but now under the Project subheading, navigate to Members and click the black + Add member button. This should allow you to select members from your organization to add to the project.\nNow that the organization, membership, and projects are set up, Step 7 is about setting up your API key. This key is, as named, the passcode that permits a member of the organization to interact with the models you pay for. Under the Project subheading, navigate to API keys and click the black + Create new secret key. This will generate a key that you can only view once, so copy and store it in a protected space. Collaborators on the project should be given this key.\nAnd the final Step 8 is to add funds to the project by navigating to the Billing section under the Organization subheading. Here you can Add to credit balance the amount of money you want to spend. The balance will be run down as you use the API, according to OpenAI’s pricing model.\nAs an added element, if there are limits you would like to set on a project, such as budgets (a maximum amount that can be spent in a month) or model limits (which models are to be used in a project), you can do this under the Project’s Limits section."
  },
  {
    "objectID": "posts/llms-text-analysis.html#elements-of-the-query",
    "href": "posts/llms-text-analysis.html#elements-of-the-query",
    "title": "LLMs for text analysis",
    "section": "Elements of the query",
    "text": "Elements of the query\nTo explain what each of these elements are\n\nModels\n\n\nTemperature, top_p, and max_tokens\n\n\nSystem instructions\n\n\nUser prompt\n\n\nAPI key"
  },
  {
    "objectID": "posts/llms-text-analysis.html#interfacing-with-the-api",
    "href": "posts/llms-text-analysis.html#interfacing-with-the-api",
    "title": "LLMs for text analysis",
    "section": "Interfacing with the API",
    "text": "Interfacing with the API\nNow that we have defined each of these elements, we can compile them as parameters in a list, which will make up the body of our API request. This API request will eventually be what we ‘send’ to the model, essentially chatting with it as if in the chat interface.\n\ninstructions &lt;- \"This is a description of a plant variety patent. If this text has information about the parents of this plant variety, report the parent names using pedigree notation (e.g. G00124 = (NS6265 × 06DSB13939) × NS6163). If one or more of the parents are reported to be proprietary or confidential (e.g. experimental lines), include the word proprietary at the start of the name (e.g. PAS1305707 = proprietary R2358D × proprietary M6948D). If the patent number is referenced, include that alongside the  parent name in curly brackets (e.g. proprietary × 25R40 {US patent number 8492625}. If there is no parent information, say 'none'.\"\n\ncontent &lt;- \"Hybrid tomato plant HM 5369 has superior characteristics of high yield and fruit quality and uniform, pear shaped fruit. The female (TOM3503PL) and male (TOM3783PL) parents were crossed to produce hybrid (F1) seeds of HM 5369. The seeds of HM 5369 can be grown to produce hybrid plants and parts thereof. The hybrid HM 5369 can be propagated by seeds produced from crossing tomato inbred line TOM3503PL with tomato inbred line TOM3783PL or vegetatively.\"\n\nbody &lt;- list(\n  model = \"gpt-4.1\",\n  max_tokens = 50,\n  temperature = 0.1,\n  top_p = 0.5,\n  messages = list(\n    list(role = \"system\",\n         content = instructions),\n    list(role = \"user\", \n         content = content)\n  ))\n\nNow that we have we will post a request to the API. The API URL we will be using for chat completions is “https://api.openai.com/v1/chat/completions”. We will also have to authorize our use of the API with our API key, which for the sake of this tutorial is not revealed. We specify that we want to encode the request in the json format, and then we send the body of our request. The result of this request is the response.\n\nresponse &lt;- POST(\n  url = \"https://api.openai.com/v1/chat/completions\", \n  add_headers(Authorization = paste(\"Bearer\", apiKey)),\n  content_type_json(),\n  encode = \"json\",\n  body =  body\n)\n\nWe can extract the content of that response, and then extract the message content from the model, providing our answer.\n\nresponse &lt;- content(response)\n\n\nanswer &lt;- trimws(response$choices[[1]]$message$content)\nanswer\n\n[1] \"HM 5369 = TOM3503PL × TOM3783PL\""
  },
  {
    "objectID": "posts/llms-text-analysis.html#scaling-queries",
    "href": "posts/llms-text-analysis.html#scaling-queries",
    "title": "LLMs for text analysis",
    "section": "Scaling queries",
    "text": "Scaling queries\n\nPut it in a function\n[FORTHCOMING]\n\n\nRate limits\nTokens per minute, requests per minute"
  },
  {
    "objectID": "posts/llms-text-analysis.html#footnotes",
    "href": "posts/llms-text-analysis.html#footnotes",
    "title": "LLMs for text analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn fact, OpenAI is in a lawsuit with the New York Times for copyright infringement, namely using copyrighted data to train its models. One main critique of LLMs then, is that they has taken text – both from private (copyrighted) and public (internet forums, blogs, etc) sources – and is now capitalizing on this incredible, arguably illegal, enclosure.↩︎"
  },
  {
    "objectID": "posts/evernote-to-md.html",
    "href": "posts/evernote-to-md.html",
    "title": "Converting Evernote notes (.enex) to markdown (.md)",
    "section": "",
    "text": "For researchers, systematically managing notes can be a game-changer for helping navigate the endless sea of knowledge the we spend our careers accumulating. There are a dozen different styles out there, which is not what I’m here to explain. Instead, this post is specific to my note-taking scenario: Free Evernote is dead, so this post is about exporting notes from Evernote into markdown files using the evernote2md package\nI have been an Evernote user for nearly a decade, particularly during my PhD (2018-2023). But as many Reddit threads will tell you, the end of 2023 marked a turning point for Evernote. Gone are the days of limitless free notes and multi-device syncing. So I finally decided to port over my notes from Evernote into a more transportable file type: from .enex of Evernote to .md files that can be used across a range of text editors. Personally, I’ve decided to migrate to Obsidian.\nWhile this post is software specific, there is a general message: try to organize notes in a way that anticipates software phase-out. I don’t want to worry about losing access to my notes because a free trial ends or not know how to even access the raw text of my notes (e.g. Apple Notes).\nSo, let’s get to it."
  },
  {
    "objectID": "posts/evernote-to-md.html#note-management",
    "href": "posts/evernote-to-md.html#note-management",
    "title": "Converting Evernote notes (.enex) to markdown (.md)",
    "section": "",
    "text": "For researchers, systematically managing notes can be a game-changer for helping navigate the endless sea of knowledge the we spend our careers accumulating. There are a dozen different styles out there, which is not what I’m here to explain. Instead, this post is specific to my note-taking scenario: Free Evernote is dead, so this post is about exporting notes from Evernote into markdown files using the evernote2md package\nI have been an Evernote user for nearly a decade, particularly during my PhD (2018-2023). But as many Reddit threads will tell you, the end of 2023 marked a turning point for Evernote. Gone are the days of limitless free notes and multi-device syncing. So I finally decided to port over my notes from Evernote into a more transportable file type: from .enex of Evernote to .md files that can be used across a range of text editors. Personally, I’ve decided to migrate to Obsidian.\nWhile this post is software specific, there is a general message: try to organize notes in a way that anticipates software phase-out. I don’t want to worry about losing access to my notes because a free trial ends or not know how to even access the raw text of my notes (e.g. Apple Notes).\nSo, let’s get to it."
  },
  {
    "objectID": "posts/evernote-to-md.html#re-create-a-skeleton-of-your-stack-and-notebook-file-structure",
    "href": "posts/evernote-to-md.html#re-create-a-skeleton-of-your-stack-and-notebook-file-structure",
    "title": "Converting Evernote notes (.enex) to markdown (.md)",
    "section": "1. Re-create a skeleton of your stack and notebook file structure",
    "text": "1. Re-create a skeleton of your stack and notebook file structure\nFirst, Evernote’s file organization is based on ‘stacks’ which hold ‘notebooks’ which hold ‘notes.’ I’m not going to play under the hood of Evernote to really understand how this is embedded, instead I’m going to keep this relatively simple and manual. Here’s a look at my stack and notebook structure. I have 8 stacks and 33 notesbooks in total.\n\nFirst, we’re going to just tell R how many stacks we have and then how many notebooks each stack contains.\n\n## Identify how many stacks you have\nn_stacks &lt;- 8\n\n## Identify the number of notebooks in each stack\n## This should be a vector of length = n_stacks, \n## each value representing n notebooks\nn_notebooks_per_stack &lt;- c(6, 7, 2, 3, 4, 4, 2, 5) \n\nWe’re then going to use these values to create a skeleton of the directory structure that we want to maintain. In total, I have 33 notebooks but they are nested differently, which is why we need to get a little creative.\n\n## Make directories for each notebook, labelled by stack-notebook number\ndir_names_pt1 &lt;- unlist(mapply(rep_len, 1:n_stacks, n_notebooks_per_stack))\ndir_names_pt2 &lt;- unlist(sapply(n_notebooks_per_stack, function(to) seq(to)))\ndir_names &lt;- paste(dir_names_pt1, dir_names_pt2, sep = \"_\")\nlength(dir_names)\n\n[1] 33\n\nhead(dir_names, n = 10)\n\n [1] \"1_1\" \"1_2\" \"1_3\" \"1_4\" \"1_5\" \"1_6\" \"2_1\" \"2_2\" \"2_3\" \"2_4\""
  },
  {
    "objectID": "posts/evernote-to-md.html#create-directory-for-evernote-export",
    "href": "posts/evernote-to-md.html#create-directory-for-evernote-export",
    "title": "Converting Evernote notes (.enex) to markdown (.md)",
    "section": "2. Create directory for Evernote export",
    "text": "2. Create directory for Evernote export\nNext, we’ll identify where we would like to export the notes. Exporting notes is going to be a manual process (Step 3), and this step is intermediate. In other words, you’ll probably delete this folder when all is said and done, so I’m just going to keep it on my Desktop.\n\n## Identify where you would like to export your notes\nexport_loc &lt;- '~/Desktop/evernotes/'\n## Create the main directory\ndir.create(export_loc)\n## Create sub-directories based on stack-notebook structure\n\nThen we will use our vector of directory names to replicate the structure we have in Evernote to fill as we manually download.\n\nsapply(paste0(export_loc, dir_names), dir.create)"
  },
  {
    "objectID": "posts/evernote-to-md.html#manually-export-evernotes",
    "href": "posts/evernote-to-md.html#manually-export-evernotes",
    "title": "Converting Evernote notes (.enex) to markdown (.md)",
    "section": "3. Manually export Evernotes",
    "text": "3. Manually export Evernotes\nHere comes the manual part: exporting notes from Evernote. From what I can tell, the largest unit you can export from Evernote is the ‘notebook,’ so I will do that for all 33 of my notebooks. I click on the three dots suggesting options and select Export notebook.\n\nI then save each notebook into it’s sub-directory. Using this method, each .enex file (containing all of the notes in the notebook) should be saves in its own sub-directory. For your first notebook in the first stack, save in sub-directory 1_1/. For the third notebook in the fifth stack, save in sub-directory 3_5. And so on."
  },
  {
    "objectID": "posts/evernote-to-md.html#create-a-destination-for-the-markdown-conversions",
    "href": "posts/evernote-to-md.html#create-a-destination-for-the-markdown-conversions",
    "title": "Converting Evernote notes (.enex) to markdown (.md)",
    "section": "4. Create a destination for the markdown conversions",
    "text": "4. Create a destination for the markdown conversions\nJust like we created an export location, we now want to create a directory for the markdown files. These are were your new notes will be, each as markdown files in their appropriate sub-directory. Again, I’m just leaving mine on my desktop for now.\n\n## Identify where you would like the markdown files to be\nmd_loc &lt;- '~/Desktop/obeliskgate/'\n## Create the directory\ndir.create(md_loc)\n## Create sub-directories to mirror the stack-notebook structure\nsapply(paste0(md_loc, dir_names), dir.create)"
  },
  {
    "objectID": "posts/evernote-to-md.html#convert-with-the-evernote2md-package",
    "href": "posts/evernote-to-md.html#convert-with-the-evernote2md-package",
    "title": "Converting Evernote notes (.enex) to markdown (.md)",
    "section": "5. Convert with the evernote2md package",
    "text": "5. Convert with the evernote2md package\nAll of this conversion is possible thanks to the evernote2md package, which relies on one function to run in the in the command line. We can run it via the system() command in R. evernote2md takes an input and output argument, which for us is the export location and md location, applied across sub-directories.\n\nsapply(dir_names, function(x) system(paste('evernote2md', \n                                           paste0(export_loc, x), \n                                           paste0(md_loc, x))))\n\nAnd that’s it! Now you should have your markdown files in each of your sub-directories."
  },
  {
    "objectID": "posts/evernote-to-md.html#optional-re-name-directories-according-to-notebook-names",
    "href": "posts/evernote-to-md.html#optional-re-name-directories-according-to-notebook-names",
    "title": "Converting Evernote notes (.enex) to markdown (.md)",
    "section": "6. Optional: re-name directories according to notebook names",
    "text": "6. Optional: re-name directories according to notebook names\nNow, you might not like the 1_1 naming structure, which is fair. Want to port over your old notebook names? Identify the notebook names from the original export locations then rename the existing filenames in your new markdown location.\n\n## Identify notebook names and pull out just the names (not numbers or extensions)\nnotebook_names &lt;- stringr::str_remove_all(list.files(export_loc, recursive = T),\n                                          \"^\\\\d{1,2}_\\\\d{1,2}\\\\/|\\\\.enex\")\n## Get the current numeric names of the markdown directories\ncurrent_names &lt;- list.files(md_loc, full.names = T)\n## Rename by appending notebook names to stack structure\nmapply(file.rename, current_names, paste(current_names, notebook_names))"
  },
  {
    "objectID": "posts/googledoc-aggiedown.html",
    "href": "posts/googledoc-aggiedown.html",
    "title": "Translating Google Docs to aggiedown LaTeX",
    "section": "",
    "text": "In 2018, Ryan Peek created the aggiedown package: “An R Markdown template using the bookdown package for preparing a PhD thesis at the University of California Davis.” This service to the PhD students of UC Davis is incredible – it lets students link their dissertation to a lovely LaTeX format that should, ideally, make your formatting process easy and compliant with the Graduate Studies formatting guidelines. So, I wanted to continue the legacy by keeping the template alive and well (you should too, by following the guide to contributing updates as necessary during use!).\nBeyond using the template, I wanted to make my dissertation-writing life as easy as possible by integrating my mode of writing (Google Docs) with the aggiedown template. As it stands, Peek proposes either writing chapters directly into the chapter Rmd files in the package, or knitting from a “child” Rmd that lives inside a specific project. While both of these options are great, working in Google Docs until the very end let me work in edits from my committee up until the last minute, and allowed me to work with my reference manager of choice (Zotero plug-in to Google Docs) instead of dealing much with BibTeX.\nSo, I created a workflow for converting a Google Doc with Zotero-embedded references into an Rmd document that will play nicely with LaTeX templates. Though I designed this for aggiedown, it is likely tweak-able for other LaTeX templates as well. This tutorial outlines what to do, however, it assumes you have read the aggiedown README file for an understanding of how that package works first."
  },
  {
    "objectID": "posts/googledoc-aggiedown.html#pre-requisites",
    "href": "posts/googledoc-aggiedown.html#pre-requisites",
    "title": "Translating Google Docs to aggiedown LaTeX",
    "section": "Pre-requisites",
    "text": "Pre-requisites\n\nSkill-level. This workflow is for someone with intermediate coding skills, in part because I didn’t bug-proof the functions very much and so it may require some tweaking. I’d say an ideal candidate is someone who understands what LaTeX is – but doesn’t really know how to use it – and understands Rmd well-enough (e.g. someone who is past the learning curve of spending hours on getting a kable just right and hardly ever gets working directory errors anymore).\n\nSoftware. Writing in Google Docs and embedding references using Zotero Google Docs plug-in. Please download the BibTex format for Zotero so that you have this option below. Downloads can happen just by navigating Zotero &gt; Settings to the box (below) searching via the ‘Get additional styles…’ link.\n\naggiedown. Please read the aggiedown documentation first, as this tutorial takes for granted a general understanding of the workflow and recommendations made in that package. It builds directly on the recommendations and formatting style of the package."
  },
  {
    "objectID": "posts/googledoc-aggiedown.html#set-up",
    "href": "posts/googledoc-aggiedown.html#set-up",
    "title": "Translating Google Docs to aggiedown LaTeX",
    "section": "Set-up",
    "text": "Set-up\n\nWriting in Google Docs. The Introduction and each Chapter is a document, so your set-up should look something like this.\n\n\n\n\n\n\n\n\n\n\n\nCoding in R Projects. Each chapter has an R project (ideally – though in reality you just need to know the filepaths of all your figures and tables). I’ll leave this to your imagination."
  },
  {
    "objectID": "posts/googledoc-aggiedown.html#doing-your-thing-in-google-docs",
    "href": "posts/googledoc-aggiedown.html#doing-your-thing-in-google-docs",
    "title": "Translating Google Docs to aggiedown LaTeX",
    "section": "Doing your thing in Google Docs",
    "text": "Doing your thing in Google Docs\nSo, you’re writing your dissertation in a Google Doc. Great. You should use docs as you wish, with only a few things to pay attention to.\n\nUse headers appropriately. Google Docs (like Word) has text related settings where you can set Titles, Header 1, Header 2, etc. These translated well to Markdown, and so we need to make sure we use these in a consistent and compatible way with the dissertation format. Your Chapter Title should be Header 1, and then all main sections (e.g. Introduction, Methods, etc.) should be Header 2. You can continue down to subsections as much as you wish. For references, please make a Header 1 titled ‘References’ at the end of the document.\nChapter marks. To get friendly chapter marks, as advised by Ryan Peek in the aggiedown guidelines, you can embed this right into the doc (e.g. [backslash]chaptermark {organic seed policy}. Also note that because this is Chapter 1, I have a set-up chunk with the packages I need to read in images and load tables. More on chunks in the next point.\n\n\n\n\n\n\n\n\n\n\n\nCode chunks. So, you’ll need to manually insert chunks, which is a trickier part. You can surely paste in your figures and tables and stuff as you write. But towards the end you’ll want to work with the model of inserting chunks into the text instead of the inserting figures and tables. For example, here I am loading in a figure, which is easy to you only need to set your chunk options and load the right path.\n\n\n\n\n\n\n\n\n\n\nTables are a little trickier. All of my tables are saved as .csv files in my projects, and I have to read them in and do some manual changes to get them right. This means sometimes renaming column names and setting some kable and kableExtra options to get my table just right. Important: be sure to include a colon (;) after each line of code so that eventually the rendering will recognize a line break in the code.\n\n\n\n\n\n\n\n\n\n\nZotero settings. You can write the dissertation with any citation format you want via Zotero (MLA, APSA, etc.), but when you’re in the end/knitting phase, you’ll want to switchto BibTex format. See above for the directions for downloading and switching the Settings for the Style Manager. Note, in text this options is very ugly, but the heart of the conversion function is making it into something usable, so deal with it.\nReferences. I said it in 1., but it bears repeating. Insert your bibliography under a Header 1 sections called “References”. The spelling a capitalization matters here, so please be conscious of that.\nMisc formatting. This conversion will recognize simple markup like bolding and italics, will ignore things like comments in the document, but it will panic a little but over highlighting. Please don’t use highlight markup. To embed links, you can use standard markdown link embedding. For citing page numbers, I have not figured out how to put this into the workflow, so I just do not link these inline citations to Zotero."
  },
  {
    "objectID": "posts/googledoc-aggiedown.html#r-chapter-projects",
    "href": "posts/googledoc-aggiedown.html#r-chapter-projects",
    "title": "Translating Google Docs to aggiedown LaTeX",
    "section": "R chapter projects",
    "text": "R chapter projects\nYou’ll have to manage your chapters as projects. However you do this, you’ll need to build in a few features.\n\nFor each chapter project, save your figures and tables in sensible places with sensible names. For example, I have a tables and figures directory for each project, and in each of those projects I have just named them by numbers. You can do whatever naming convention you wish, you’ll just need to know the path for later.\n\n\n\n\n\n\n\n\n\n\n\nIn each chapter project, create a manuscript folder. For example, in this project I have a ch1-manuscript/ directory. This will be a path you use for later, as it is where the functions I’ve made will create your chapter Rmd documents."
  },
  {
    "objectID": "posts/googledoc-aggiedown.html#aggiedown",
    "href": "posts/googledoc-aggiedown.html#aggiedown",
    "title": "Translating Google Docs to aggiedown LaTeX",
    "section": "aggiedown",
    "text": "aggiedown\nYou’ll also have a project from following the directions to clone the aggiedown package. A few additions here can help the workflow.\n\nIntro directory. In the aggiedown project, create an intro/ directory. This is where your introduction will be rendered from.\nScript to prep chapters. Create a prep_papers.R script in the main directory. This is where you will call in two functions from my open source function repo, woodshop. These functions are prep_diss_chapters and auto_bib.\n\n\ndevtools::install_github('liza-wood/woodshop')\n\nintrourl &lt;- 'https://docs.google.com/document/d/1rhGeXrAtk8hdtB1uymg1WwaD_P3Xv3oz_lm1sl4Qnao/edit#'\nintropath &lt;- '~/Documents/Davis/R-Projects/aggiedown_dissertation/intro/'\n\nch1url &lt;- 'https://docs.google.com/document/d/1T9K308unavSzrx7A4ZsXRTScazhfeAVHGT-rtawP1x4/edit'\nch1path &lt;- '~/Documents/Davis/R-Projects/organicseed_adoption/ch1-manuscript/'\n\nch2url &lt;- 'https://docs.google.com/document/d/1wsDqDBgijoXFMuOlyHa3hSlUwcNLpUWnRna11r8dxzs/edit'\nch2path &lt;- '~/Documents/Davis/R-Projects/osisn_spatial/ch2-manuscript/'\n\nch3url &lt;- 'https://docs.google.com/document/d/1OWRFWWUNgEmn2VDEcj6bS8uc_lL1VnteS5jK-_YyhII/edit'\nch3path &lt;- '~/Documents/Davis/R-Projects/osisn_processes/ch3-manuscript/'\n\nwoodshop::prep_diss_chapters(introurl, intropath, intro = T)\nwoodshop::prep_diss_chapters(ch1url, ch1path)\nwoodshop::prep_diss_chapters(ch2url, ch2path)\nwoodshop::prep_diss_chapters(ch3url, ch3path)\n\nwoodshop::auto_bib_file(intropath, ch1path, ch2path, ch3path)\n\nAfter running this code in the prep_papers.R file, the directories created in each chapter project should fill out.\n\n\n\n\n\n\n\n\n\n\nKnit children. In each chapter .Rmd built in to the aggiedown package (e.g. 01-chap1.Rmd), you’ll want to set them up so that they knit from the Rmds created in the previous step. In each chapter Rmd, you can mimic the following code chunk to draw it in. This is the only thing you need in each chapter Rmd.\n\n\n\n\n\n\n\n\n\n\n\nFinal touches. At the very end (i.e. you’re not going to run prep_papers.R any more) you can make any specific manual edits that you’d like by directly editing the chapter’s Rmd in the chapter project. For example, one of the main bugs that I hadn’t solved relates to footnotes. They overwrite one another because they do not get continuously numbered. If you have footnotes, you’ll need to run each chapter Rmd and make the numbers continous, otherwise you’ll have issues.\n\nOnce you’ve followed these steps, you should be able to knit from the index.Rmd file in the aggiedown package and render your thesis, stored as thesis.pdf in the book/ directory of the project.\nA few pages are below as an example. On page v., the rendering takes all of the first and second headers from the Google Doc into the table of contents. On page 5, you see the chapter title, introduction header, and then that references are embedded and linked to the bibliography, formatted based on how I specified the csl. Then on pages 16 and 19, the figure and table I showed as chunks in text are fitting in the final rendering. Note also that the page header alternatives between chapter mark on even pages (e.g. CHAPTER 1. ORGANIC SEED POLICY) and subsection headers on odd pages (e.g. 3. METHODS)."
  },
  {
    "objectID": "posts/ol_sequential.html",
    "href": "posts/ol_sequential.html",
    "title": "Sequential ordered logits",
    "section": "",
    "text": "In a previous tutorial I went over two packages for ordered logits – polr() and vglm() – and walked through some of the basics for running models in both. In those models I focused on more classical cumulative ratio models, but now I am going to introduce continuation ratio models, also called a sequential model. A good review can be found in Chapters 4 and 5 of O’Connell’s Logistic Regression Models for Ordinal Response Variables. In short though, the added value is this: Unlike the more common cumulative ordered logistic regression (i.e. ordered logit), where the model is estimating the likelihood of being at or above a certain category (e.g. P(Y&gt;=2)), a sequential logit evaluates the likelihood of being in a particular level against the likelihood of being in the level just below it (e.g. P[Y&gt;2|Y&gt;=2]).\nLet’s see how we can run this models using the VGAM package.\n\nlibrary(VGAM) \n\nLoading required package: stats4\n\n\nLoading required package: splines\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nI’ll be using the same data set described in the previous tutorial, farmer interest in adopting a new technology.\n\nhead(df)\n\n  interest acres income education econ.value envt.value info.count\n1      Not  4.84     NA        NA       4.33        3.6          1\n2 Somewhat  3.01      1         5       4.00        3.0          1\n3 Somewhat  4.17      3         2         NA         NA          1\n4     &lt;NA&gt;  4.40      3         3       5.00         NA          3\n5      Not  2.44      4         7       4.67        3.6          9\n6     Very  6.64      2         7       2.00        4.2          5\n\n\nIf we wanted to run a more classic, cumulative model, the model specifications would be as follows, namely specifying our family as “cumulative”. Note we are also specifying that parallel = T for now, indicating that we assume that the slopes between stages are parallel, and that we reverse the order of our stages because vglm() oddly defaults to descending order.\n\ncumltv.model &lt;- vglm(ordered(interest) ~ acres + income + education + \n                         econ.value + envt.value + info.count, data = df, \n                       family=cumulative(parallel = T, reverse = T))\nsummary(cumltv.model)\n\n\nCall:\nvglm(formula = ordered(interest) ~ acres + income + education + \n    econ.value + envt.value + info.count, family = cumulative(parallel = T, \n    reverse = T), data = df)\n\nCoefficients: \n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept):1  0.88002    0.54529   1.614  0.10656    \n(Intercept):2 -1.01208    0.54711  -1.850  0.06433 .  \nacres          0.06972    0.06660   1.047  0.29517    \nincome        -0.18702    0.07119  -2.627  0.00861 ** \neducation     -0.02324    0.06035  -0.385  0.70024    \necon.value    -0.50212    0.10343  -4.855 1.20e-06 ***\nenvt.value     0.57938    0.09616   6.025 1.69e-09 ***\ninfo.count     0.14728    0.03288   4.479 7.51e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: logitlink(P[Y&gt;=2]), logitlink(P[Y&gt;=3])\n\nResidual deviance: 1002.928 on 1010 degrees of freedom\n\nLog-likelihood: -501.4641 on 1010 degrees of freedom\n\nNumber of Fisher scoring iterations: 5 \n\nNo Hauck-Donner effect found in any of the estimates\n\n\nExponentiated coefficients:\n     acres     income  education econ.value envt.value info.count \n 1.0722115  0.8294304  0.9770324  0.6052435  1.7849227  1.1586799 \n\n\nThe switch to the sequential model takes just a change in the family, now specifying it as “cratio”, and no longer reversing the order.\n\nseqntl.model &lt;- vglm(ordered(interest) ~ acres + income + education + \n                         econ.value + envt.value + info.count, data = df, \n                       family=cratio(parallel = T))\nsummary(seqntl.model)\n\n\nCall:\nvglm(formula = ordered(interest) ~ acres + income + education + \n    econ.value + envt.value + info.count, family = cratio(parallel = T), \n    data = df)\n\nCoefficients: \n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept):1  1.11877    0.49400   2.265   0.0235 *  \n(Intercept):2 -0.28988    0.49660  -0.584   0.5594    \nacres          0.05510    0.05989   0.920   0.3576    \nincome        -0.16453    0.06407  -2.568   0.0102 *  \neducation     -0.03270    0.05432  -0.602   0.5472    \necon.value    -0.46247    0.09476  -4.881 1.06e-06 ***\nenvt.value     0.49286    0.08635   5.708 1.14e-08 ***\ninfo.count     0.13283    0.03000   4.427 9.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: logitlink(P[Y&gt;1|Y&gt;=1]), logitlink(P[Y&gt;2|Y&gt;=2])\n\nResidual deviance: 1004.007 on 1010 degrees of freedom\n\nLog-likelihood: -502.0035 on 1010 degrees of freedom\n\nNumber of Fisher scoring iterations: 5 \n\nNo Hauck-Donner effect found in any of the estimates\n\n\nExponentiated coefficients:\n     acres     income  education econ.value envt.value info.count \n 1.0566490  0.8482910  0.9678279  0.6297238  1.6369976  1.1420599 \n\n\nHere we see that the results between the two models are quite similar, though the effect sizes are slightly smaller in the sequential model with less significance. The main difference is in the conditions of the linear predictors (as noted above). Deciding which family of ordered logits is right for your model may depend on model fit, or perhaps theoretically with the nature of the variable. If you believe that the stages you are evaluating are predicated on going through the previous stage, then a sequential model might be for you.\nAlso important to note in vglm() is that we cannot rely on the brant() function as we can with polr models. So, testing our proportional odds assumption (that is, the assumption that parallel = T), takes a bit of manual work. Let’s first specify the model without making the proportional odds assumption.\n\nseqntl.model.nonprop = vglm(ordered(interest) ~ acres + income + education + \n                         econ.value + envt.value + info.count, data = df, \n                          family=cratio(parallel = F))\nsummary(seqntl.model.nonprop)\n\n\nCall:\nvglm(formula = ordered(interest) ~ acres + income + education + \n    econ.value + envt.value + info.count, family = cratio(parallel = F), \n    data = df)\n\nCoefficients: \n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept):1 -0.883092   0.662736  -1.332 0.182698    \n(Intercept):2  2.230679   0.792002   2.817 0.004855 ** \nacres:1        0.093624   0.085864   1.090 0.275547    \nacres:2        0.006717   0.086027   0.078 0.937768    \nincome:1      -0.223318   0.093201  -2.396 0.016572 *  \nincome:2      -0.109974   0.091128  -1.207 0.227502    \neducation:1    0.029837   0.076562   0.390 0.696746    \neducation:2   -0.092454   0.079330  -1.165 0.243841    \necon.value:1  -0.154035   0.129175  -1.192 0.233081    \necon.value:2  -0.767367   0.144987  -5.293 1.21e-07 ***\nenvt.value:1   0.633108   0.114018   5.553 2.81e-08 ***\nenvt.value:2   0.248064   0.138626   1.789 0.073543 .  \ninfo.count:1   0.110310   0.045887   2.404 0.016220 *  \ninfo.count:2   0.153613   0.040627   3.781 0.000156 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: logitlink(P[Y&gt;1|Y&gt;=1]), logitlink(P[Y&gt;2|Y&gt;=2])\n\nResidual deviance: 980.3759 on 1004 degrees of freedom\n\nLog-likelihood: -490.188 on 1004 degrees of freedom\n\nNumber of Fisher scoring iterations: 5 \n\nNo Hauck-Donner effect found in any of the estimates\n\n\nExponentiated coefficients:\n     acres:1      acres:2     income:1     income:2  education:1  education:2 \n   1.0981470    1.0067392    0.7998604    0.8958571    1.0302870    0.9116913 \necon.value:1 econ.value:2 envt.value:1 envt.value:2 info.count:1 info.count:2 \n   0.8572417    0.4642337    1.8834549    1.2815419    1.1166244    1.1660390 \n\n\nA first good sign is that we did not encounter any errors. In some cases, vglm() cannot fit the non-proportional model if it really isn’t suited. If the model runs, however, we still want to check the assumption. Below is a method for testing whether or not the model overall meets the assumption (though it does not provide p-values for specific variables). We calculate the p-value against the hypothesis that the proportion odds assumption is correct.\n\ndev &lt;- deviance(seqntl.model) - deviance(seqntl.model.nonprop)\ndegf &lt;- seqntl.model@df.residual - seqntl.model.nonprop@df.residual\np.value &lt;- 1 - pchisq(q = dev , df = degf)\np.value\n\n[1] 0.0006104421"
  },
  {
    "objectID": "posts/ol_partialprop.html",
    "href": "posts/ol_partialprop.html",
    "title": "Ordered logits introduction",
    "section": "",
    "text": "Logistic regression models that regress on an ordinal dependent variable (think: Likert scale responses, course grades, categories that have a natural sequence, etc.) will use an ordered logit. Ordered logits are often run in R using polr in the MASS package, however the vglm function in the VGAM package offers a more advanced suite of options. This tutorial walks through setting up ordered logits with each function.\nFirst, let’s load in our packages.\nFor this example, suppose we are interested in the question: What drives farmers’ interest in technology x? We have data on farmers’ education and income, their farm size, as well as their attitudes towards things like earning money, protecting the environment, as well has how connected they are to different information sources. We are interested in the dependent variable: Farmer interest in the technology, scored as no interest (None), some interest (Somewhat), or very interested (Very) (1-3). Let’s inspect these data.\n\nhead(df)\n\n  interest acres income education econ.value envt.value info.count\n1      Not  4.84     NA        NA       4.33        3.6          1\n2 Somewhat  3.01      1         5       4.00        3.0          1\n3 Somewhat  4.17      3         2         NA         NA          1\n4     &lt;NA&gt;  4.40      3         3       5.00         NA          3\n5      Not  2.44      4         7       4.67        3.6          9\n6     Very  6.64      2         7       2.00        4.2          5\n\n# Make sure your dependent variable is in the right order\ndf$interest = factor(df$interest, c('Not', 'Somewhat', 'Very'))\n\nNote: These data have been inspired by farmer survey data sets, but have simulated values and therefore do not reflect any actual farmers.\n\npolr\nMost ordered logit models, such as those run with the defaults of the polr function, assume a cumulative link function. A cumulative link function looks at the probability of being at one level, compared to all of the levels below them (e.g. P(Y&gt;=2)). This is the only kind of link function available in polr, which is important to keep in mind when later comparing it to vglm.\nNow, let’s take a look at our model, where we are interested in seeing how farm operator/operation variables, attitudes, and information affect a farmer’s interest in a technology.\n\npolr.model &lt;- polr(interest ~ acres + income + education + econ.value + \n                     envt.value + info.count, data = df, Hess = T) \n# We use Hess = T in order to return the Hessian matrix, which allows us to look at the model's summary\nsummary(polr.model)\n\nCall:\npolr(formula = interest ~ acres + income + education + econ.value + \n    envt.value + info.count, data = df, Hess = T)\n\nCoefficients:\n              Value Std. Error t value\nacres       0.06972    0.06798  1.0256\nincome     -0.18702    0.07160 -2.6120\neducation  -0.02324    0.06123 -0.3795\necon.value -0.50213    0.10715 -4.6861\nenvt.value  0.57938    0.09664  5.9950\ninfo.count  0.14728    0.03291  4.4759\n\nIntercepts:\n              Value   Std. Error t value\nNot|Somewhat  -0.8801  0.5639    -1.5606\nSomewhat|Very  1.0120  0.5668     1.7855\n\nResidual Deviance: 1002.928 \nAIC: 1018.928 \n(106 observations deleted due to missingness)\n\n\nThe summary of the polr function returns the coefficients, standard errors and t-values. These results indicate that economic attitudes and income are significantly negatively correlated with technology interest, and environmental attitudes and information sources is positively correlated. Ordered logit coefficient values, in their raw form, is their logit value, where the strength of the effect is hard to interpret. Typically, estimates are exponentiated to be read as odds.\n\npolr.model$coefficients\n\n      acres      income   education  econ.value  envt.value  info.count \n 0.06972332 -0.18701627 -0.02323590 -0.50213351  0.57937593  0.14728163 \n\n\nWe can also calculate the p-value manually for an easier read of results, and bind it to the summary table.\n\n# Save an object as the coefficient table\nctable &lt;- coef(summary(polr.model))\n# Calculate and store p values\np &lt;- round((pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2),3)\n# Bind these objects into one table\ntable &lt;- cbind(ctable, \"p value\" = p)\n\ntable\n\n                    Value Std. Error    t value p value\nacres          0.06972332 0.06798285  1.0256016   0.305\nincome        -0.18701627 0.07159954 -2.6119757   0.009\neducation     -0.02323590 0.06122978 -0.3794869   0.704\necon.value    -0.50213351 0.10715380 -4.6861009   0.000\nenvt.value     0.57937593 0.09664368  5.9949696   0.000\ninfo.count     0.14728163 0.03290579  4.4758571   0.000\nNot|Somewhat  -0.88005549 0.56391926 -1.5606055   0.119\nSomewhat|Very  1.01204550 0.56681362  1.7854996   0.074\n\n\nBefore relying too much on these results, however, one assumption of ordered logits that ought to be tested is the proportional odds assumption. What is assumed here is that the slopes from level to level of the dependent variable are relatively similar, allowing us to use only one coefficient per independent variable. The Brant test was developed to test this assumption, and the brant package and function allow us to quickly test this with polr model objects.\n\nbrant(polr.model)\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     33.3    6   0\nacres       0.43    1   0.51\nincome      0.35    1   0.55\neducation   1.47    1   0.22\necon.value  19.17   1   0\nenvt.value  1   1   0.32\ninfo.count  1.46    1   0.23\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\nThis test evaluates whether or not the proportional odds assumption has been met, where the null hypothesis is that the assumption has been met. In this case, a p-value &lt; 0.05 is cause to reject the null hypothesis, meaning that reject the proportional odds assumption. Based on these outputs, we see that the overall (Omnibus) model fails the proportional odds test, where probability = 0, and this is particularly driven by economic values, where the p-value is \\(1.2^{-5}\\).\nA failed proportional odds assumption calls for a non-proportional ordered logit, or at least partial proportional model. However, polr doesn’t allow us to do this, which is where we switch to vglm.\n\n\nvglm\nThe vglm function can model more advanced ordinal regressions – non-proportional and partial proportional, as well as different link functions. However, it requires a few more model specifications than the polr model. We will explore these using the same data as above.\nFirst, you need to specify the family of the link function. As noted above, the default family for polr and most people thinking about ordered logits is the cumulative link function, so for now we will leave it at that with family = cumulative. (See my post on the sequential ordered logit using the cratio link function). We also get to specify whether or not the parallel odds assumption, also called the proportional odds assumption, has been met with parallel = T/F. Last, the default in vglm is to reverse the order of the ordered dependent variable, so most intuitive interpretations require that the order be reversed with reverse = T. Below are examples of running the proportional, non-proportional, and partial proportional models. Note that the partial proportional model is specified by using a ~ followed by the variable(s) that you want to be left not proportional.\n\n# a proportional model\nvgam.model.prop &lt;- vglm(ordered(interest) ~ acres + income + education + \n                         econ.value + envt.value + info.count, data = df, \n                       family=cumulative(parallel = T, reverse = T))\n\n# a non-proportional model\nvgam.model.nonprop &lt;- vglm(ordered(interest) ~ acres + income + education + \n                            econ.value + envt.value + info.count,  data = df, \n                          family=cumulative(parallel = F, reverse = T))\n\n# a partial proportional model\nvgam.model.partialprop &lt;- vglm(ordered(interest) ~ acres + income + education + \n                                econ.value + envt.value + info.count, data = df, \n                              family=cumulative(parallel = F ~ econ.value, reverse = T))\n\nWhile polr model objects of the S3 class, as are most objects in R, vglm models are S4. In this case, while some functions operate the same, others vary. For instance, you can use the same summary() function.\n\n# vagm models provide p-values immediately\nsummary(vgam.model.partialprop)\n\n\nCall:\nvglm(formula = ordered(interest) ~ acres + income + education + \n    econ.value + envt.value + info.count, family = cumulative(parallel = F ~ \n    econ.value, reverse = T), data = df)\n\nCoefficients: \n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept):1 -0.46902    0.59324  -0.791  0.42918    \n(Intercept):2 -0.20922    0.57118  -0.366  0.71415    \nacres          0.06150    0.06709   0.917  0.35934    \nincome        -0.19036    0.07184  -2.650  0.00806 ** \neducation     -0.01948    0.06099  -0.319  0.74947    \necon.value:1  -0.14429    0.11887  -1.214  0.22481    \necon.value:2  -0.66601    0.11258  -5.916 3.30e-09 ***\nenvt.value     0.54720    0.09669   5.659 1.52e-08 ***\ninfo.count     0.14946    0.03316   4.507 6.57e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: logitlink(P[Y&gt;=2]), logitlink(P[Y&gt;=3])\n\nResidual deviance: 982.8936 on 1009 degrees of freedom\n\nLog-likelihood: -491.4468 on 1009 degrees of freedom\n\nNumber of Fisher scoring iterations: 8 \n\nNo Hauck-Donner effect found in any of the estimates\n\n\nExponentiated coefficients:\n       acres       income    education econ.value:1 econ.value:2   envt.value \n   1.0634294    0.8266641    0.9807110    0.8656360    0.5137545    1.7284071 \n  info.count \n   1.1612031 \n\n\nAt the bottom of this summary, coefficients are already presented in exponentaited form. Still, it is useful to know how to manipulate this model object to get out the values of interest. In a polr model you can use a $ sign after the model object to get just the coefficients, but in vglm you use the @ sign after the model summary object to pull out a variety of the model parts.\n\nvgam.summary &lt;- summary(vgam.model.partialprop)\n# coef3 pulls out the whole model output table\nvgam.summary@coef3\n\n                 Estimate Std. Error    z value     Pr(&gt;|z|)\n(Intercept):1 -0.46901843 0.59324098 -0.7906036 4.291754e-01\n(Intercept):2 -0.20922021 0.57118110 -0.3662940 7.141457e-01\nacres          0.06149894 0.06709280  0.9166250 3.593392e-01\nincome        -0.19035684 0.07184225 -2.6496501 8.057517e-03\neducation     -0.01947749 0.06099434 -0.3193327 7.494742e-01\necon.value:1  -0.14429075 0.11887181 -1.2138349 2.248108e-01\necon.value:2  -0.66600982 0.11257994 -5.9158837 3.300984e-09\nenvt.value     0.54720023 0.09669496  5.6590357 1.522259e-08\ninfo.count     0.14945660 0.03316010  4.5071221 6.571281e-06\n\n# coefficient pulls out only the coefficients\nvgam.summary@coefficients\n\n(Intercept):1 (Intercept):2         acres        income     education \n  -0.46901843   -0.20922021    0.06149894   -0.19035684   -0.01947749 \n econ.value:1  econ.value:2    envt.value    info.count \n  -0.14429075   -0.66600982    0.54720023    0.14945660 \n\n\nIf you want to neatly compile a table, including exponentiated coefficients, confidence intervals, and p-values, the following code helps compile this.\n\n# Calculate exponentiated coefficients (odds) using the model summary object\nodds &lt;- data.frame(exp(vgam.summary@coefficients))\n# Extrac p-value\np &lt;- data.frame(vgam.summary@coef3[,4])\n# Calculate the confidence intervals using the original model object\nci &lt;- data.frame(exp(confint(vgam.model.partialprop)))\n# Combine and round \nvglm.table &lt;- round(cbind(odds, ci, p),3)\ncolnames(vglm.table) &lt;- c(\"Estimate\", \"LCI\", \"UCI\", \"p-value\")\n\nvglm.table\n\n              Estimate   LCI   UCI p-value\n(Intercept):1    0.626 0.196 2.001   0.429\n(Intercept):2    0.811 0.265 2.485   0.714\nacres            1.063 0.932 1.213   0.359\nincome           0.827 0.718 0.952   0.008\neducation        0.981 0.870 1.105   0.749\necon.value:1     0.866 0.686 1.093   0.225\necon.value:2     0.514 0.412 0.641   0.000\nenvt.value       1.728 1.430 2.089   0.000\ninfo.count       1.161 1.088 1.239   0.000\n\n\nFor more on ordered logit model, see posts on:\n\nSequential ordered logit\nPredicted probabilities with ordered logits"
  },
  {
    "objectID": "posts/sen_motifr.html",
    "href": "posts/sen_motifr.html",
    "title": "Social-ecological networks with motifR: Organizing and analyzing data",
    "section": "",
    "text": "This post briefly covers how to go about organizing data for social-ecological network (SEN) analysis and running some of the main functions in Angst & Seppelt’s motifR (2020).\nFirst, lets’ load in all of the packages we’ll need. (Note: We use both igraph and network, which can conflict with one another, though not in this the code shared with this post).\nlibrary(igraph)\nlibrary(network)\nlibrary(motifr)\nlibrary(statnet)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/sen_motifr.html#note-reticulate",
    "href": "posts/sen_motifr.html#note-reticulate",
    "title": "Social-ecological networks with motifR: Organizing and analyzing data",
    "section": "Note: reticulate",
    "text": "Note: reticulate\nmotifR relies on the reticulate package in R to run over python. reticulate is great but can be annoying if python is installed in several places or you you have path issues. This may not be necessary, but if you run into issues, one thing to do is to check your environment and make sure that your path is to python on your computer.\n\n# Check out your system environment here\nSys.getenv() \n# you want to see something like: RETICULATE_PYTHON_FALLBACK = /Users/[[NAME]]/anaconda3/bin/python3\n## If it is not, you can set this yourself, eg below\nSys.setenv(RETICULATE_PYTHON_FALLBACK = '/Users/[[NAME]]/anaconda3/bin/python3')"
  },
  {
    "objectID": "posts/sen_motifr.html#motifs",
    "href": "posts/sen_motifr.html#motifs",
    "title": "Social-ecological networks with motifR: Organizing and analyzing data",
    "section": "Motifs",
    "text": "Motifs\nAssuming reticulate isn’t creating any issues, we can look at the different motifs available in the motifR package and their names. The code below will open a new window that allows you to select through different motifs and see them visually. If you cannot see them, trying making your pop-out window larger.\n\nexplore_motifs() # opens interactive window\n\nAnd then we can also see these motifs in our own network.\n\nshow_motif(\"1,2[II.C]\", net = SEN_net, directed = FALSE, label = TRUE) + \n  labs(title = \"1,2[II.C]\")\nshow_motif(\"2,2[I.D]\", net = SEN_net, directed = FALSE, label = TRUE) + \n  labs(title = \"2,2[I.D]\")\nshow_motif(\"2,2[V.D]\", net = SEN_net, directed = FALSE, label = TRUE) + \n  labs(title = \"2,2[V.D]\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd then use the count_motifs function to look at the counts of different motifs.\n\n## It may be easier just to make a list of the motifs you are interested in \n## because you will reference these often\nmotifs_of_interest &lt;- list(\"1,2[II.C]\", \"2,2[I.D]\", \"2,2[V.D]\")\ncount_motifs(SEN_net, lvl_attr = \"sesType\",\n             motifs = motifs_of_interest,\n             directed = FALSE)\n\nProcessing 1,2 motifs\n[ Classifying 302 gadgets ................................................... ]\n[ ########################################################################### ]\nProcessing 2,2 motifs\n[ Classifying 302 gadgets ................................................... ]\n[ ########################################################################### ]\n\n\n              motif count\n1,2[II.C] 1,2[II.C]    11\n2,2[I.D]   2,2[I.D]    20\n2,2[V.D]   2,2[V.D]     1"
  },
  {
    "objectID": "posts/sen_motifr.html#simulate-networks-erdos-renyi-model",
    "href": "posts/sen_motifr.html#simulate-networks-erdos-renyi-model",
    "title": "Social-ecological networks with motifR: Organizing and analyzing data",
    "section": "Simulate networks, Erdos-Renyi model",
    "text": "Simulate networks, Erdos-Renyi model\nOnce we have the counts of the network motifs, we want to know whether or not these motifs are observed more or less than would be expected, given a baseline model. We can use the simulate_baseline function to do this.\nThe baseline model against which we compare can vary. One simple baseline is a random, Erdos-Renyi graph. We’ll start with this as an example to see how the function works.\n\nn_iter &lt;- 50 # number of iterations: low for example but recommended &gt;1000\nsim_random &lt;- simulate_baseline(net = SEN_net, \n                                motifs = motifs_of_interest,\n                                n = n_iter,\n                                lvl_attr = \"sesType\",\n                                model = \"erdos_renyi\" # default\n                                )\n\nThe result is a data frame with the counts of each motif for however many simulated graphs there are.\n\nhead(sim_random)\n\n  1,2[II.C] 2,2[I.D] 2,2[V.D]\n1         7       24        2\n2         7       16        0\n3         9        7        0\n4        11       18        4\n5        13       24        3\n6         9       26        1"
  },
  {
    "objectID": "posts/sen_motifr.html#simulate-networks-ergm",
    "href": "posts/sen_motifr.html#simulate-networks-ergm",
    "title": "Social-ecological networks with motifR: Organizing and analyzing data",
    "section": "Simulate networks, ERGM",
    "text": "Simulate networks, ERGM\nNow that we understand simulate_baseline, we can use a more complex baseline model. Rather than use a random graph, which most graphs are likely to be significantly different from, we can simulate networks using Exponential Random Graph Models (ERGMs).\nFor SENs, however, we need to think about what kind of ERGM makes sense as baseline. Sometimes, ecological networks are ‘fixed’, meaning that ecological relationships are not a matter of choice. In an example, the interdependence between hydrological areas are fairly fixed (barring engineering) because water flows are dependent on the geomorphology of a place. In these scenarios, we can estimate a ‘partial’ ERGM, using only the social level, given that this is the only level really able to change.\nTo do this, we make a subgraph of our SEN to include just the social actors, and specify an ERGM.\n\n## So we can create a subgraph of just the social level\nactor_net &lt;- motifr::induced_level_subgraph(SEN_net, \n                                            level = 1, \n                                            lvl_attr = \"sesType\")\n### And then specify an ergm for the actors\nactor_ergm &lt;- ergm(actor_net ~ density + \n                     nodematch(\"type\", diff=TRUE) +\n                     nodefactor(\"type\") + \n                     gwdegree(.5, fixed = TRUE) +\n                     gwesp(.25, fixed=TRUE))\n\nThen we use this partial ERGM as the baseline model for our simulations.\n\n## Now, instead of model = erdos renyi, model = 'partial-ergm', \n## specify what level the ergm is partial for, and the ergm itself\nsim_ergm_p &lt;- simulate_baseline(SEN_net,\n                                motifs_of_interest, \n                                n = n_iter, \n                                lvl_attr = \"sesType\",\n                                model = \"partial_ergm\",\n                                level = 1,\n                                ergm_model = actor_ergm\n                                )\n\nIn a case where ecological nodes are not fixed (e.g. foraging behavior), we can also run and set a full ERGM as your baseline.\n\nse_ergm &lt;- ergm(SEN_net ~ edges + \n                          nodefactor(\"sesType\") +\n                          gwdegree(.5, fixed = TRUE) +\n                          gwesp(.25, fixed=TRUE))\n\n\nsim_ergm &lt;- simulate_baseline(SEN_net,\n                                motifs_of_interest, \n                                n = n_iter, \n                                lvl_attr = \"sesType\",\n                                model = \"ergm\",\n                                ergm_model = se_ergm\n                                )"
  }
]